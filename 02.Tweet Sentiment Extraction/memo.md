# Notebook
* [Twitter sentiment Extaction-Analysis,EDA and Model](https://www.kaggle.com/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model)
* [TSE2020 - RoBERTa (CNN) - Outlier Analysis, 3chr](https://www.kaggle.com/vbmokin/tse2020-roberta-cnn-outlier-analysis-3chr)

# Discussion
* [TensorFlow roBERTa](https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143281)<br>
HuggingFaceのTF roBERTaベースモデルについて
* [Why roBERTa?](https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/156848)<br>
RoBERTaのTweet Sentiment Extractionコンペへの有効性について
* [Why no Pre-processsing?](https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/157094) <br>
BERT系のモデルで見出し語化、ストップワード除去等を実施しない件について
<!-- * [Analysis of 3 or more repetitions in predictions ("goood"-problem)](https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/156720)<br>
goood等の3文字以上同じアルファベットが続く問題について -->

# Others
* [pythonのtokenizersのByteLevelBPETokenizerを利用してみた](https://www.hirayuki.com/kaggle-zakki/python-tokenizers)<br>
* [(Part 1) tensorflow2でhuggingfaceのtransformersを使ってBERTを文書分類モデルに転移学習する](https://tksmml.hatenablog.com/entry/2019/10/22/215000)
* [(Part 2) tensorflow 2 でhugging faceのtransformers公式のBERT日本語学習済みモデルを文書分類モデルにfine-tuningする](https://tksmml.hatenablog.com/entry/2019/12/15/090900)
* [BERTの精度を向上させる手法10選](https://qiita.com/YuiKasuga/items/343309257da1798c1b63)
